# 25-1_SPAM_SMS_classifier

본 프로젝트는 한국어 문자 데이터를 입력으로 받아 **스팸 여부**와 **스팸 유형(type)** 을 분류하는 모델을 제공합니다. 형태소 기반 전처리, 직접 구축한 토큰 사전, 희귀 단어 제거, 길이 이상치 필터링을 포함한 전처리 파이프라인을 사용하며, 모델은 **Embedding + Bidirectional LSTM** 구조로 구현됩니다.

---

## **1. 데이터 구성**

입력 데이터는 다음 컬럼으로 구성됩니다.

| Column      | 설명                           |
| ----------- | ---------------------------- |
| `text`      | 문자 내용                        |
| `is_spam`   | 0(일반), 1(스팸)                 |
| `type`      | 스팸 유형(일반=0, 광고·금융·도박 등은 1~N) |
| `generated` | 생성 여부                      |

데이터는 실제 문자 수집본, KT 스팸트랩 데이터, 스마트치안 스팸 데이터로 구성되며, 총 21664건의 SMS 문자를 활용했습니다.

---

## **2. 전처리 파이프라인**


### **1) 형태소 단위 토큰화**

* KoNLPy 형태소 분석기로 문장을 형태소 리스트로 분리

### **2) 불용어 제거**

* 조사·접속사·감탄사 등 의미 기여도가 낮은 토큰을 제거
* Notebook에서는 직접 정의한 stopword 리스트를 사용

### **3) 희귀 단어 제거**

* 전체 등장 빈도 ≤ 3인 단어를 vocabulary에서 제외
* 제거 후 남은 단어들로 최종 토큰 사전을 구성

### **4) 문자열 길이 이상치 제거**

* 전체 길이 분포 기준 상위 1%에 해당하는 초장문 메시지를 제거
* 전처리 과정에서 길이가 0인 샘플도 제거

### **5) 정수 인코딩 + padding**

* 앞서 구축한 vocabulary로 각 형태소를 정수 ID로 변환
* Notebook에서 산출된 `max_len` 값을 기준으로 padding을 고정

전처리 이후 생성된 데이터는 학습·검증·테스트 전체에서 동일한 파이프라인을 사용

---

## **3. 학습 데이터 구성 방식**

* 스팸과 일반 문자의 비율이 높게 기울어져 있기 때문에, Notebook에서는 **스팸:일반 = 1:1** 비율로 재구성하여 학습
* 이 방식은 모델이 특정 클래스에 집중되는 현상을 방지하기 위한 조치이며, 모든 실험에서 동일하게 유지
---

## **4. 최종 모델 구조**

스팸이라고 판단된 메시지를 세부 유형으로 분류합니다.

```
Embedding(vocab_size, embed_dim)
→ Bidirectional LSTM(hidden_dim)
→ Dropout
→ Dense(num_classes, activation='softmax')
```

* 클래스: 광고, 금융, 성인, 주식/투자, 도박, 정치 등
* 손실 함수: categorical_crossentropy
* label encoding 후 one-hot 변환하여 학습

데이터 특성상 ‘기타’, ‘대리운전’ 클래스는 성능 저하가 있어 제거하거나 별도 정의가 필요.

---

## **5. 성능 요약**

* 이진 분류 모델은 스팸/일반을 명확히 구분하는 수준의 안정적인 예측을 제공
* 다중 분류 모델은 의미 패턴이 뚜렷한 클래스(도박·정치 등)에서 높은 precision을 보임
* 데이터 수가 부족하거나 정의가 모호한 클래스는 예측 불확실성이 증가함


---

## **8. 팀 정보**

* 이지우
* 장동환
* 나정연
* 이채윤
